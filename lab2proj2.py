# -*- coding: utf-8 -*-
"""lab2proj2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OcZ-hiZs9_LsvaS9_LNuC2I1rphN_vAu
"""

from google.colab import files, drive
drive.mount('/content/drive')

"""#1. Dataset Loading and Data Inspection"""

# Import libraries
from keras.models import Sequential
from keras.layers import Dense
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy 
import pandas as pd
import warnings
import re
warnings.filterwarnings("ignore")
numpy.random.seed(8)

# Import dataset
fname = '/content/drive/MyDrive/lab2/tweets_train_validation.txt' 
tweet = pd.read_csv(fname, encoding='iso-8859-1', lineterminator='\n') # Read txt file
tweet.isnull().values.any() # Check if the dataset contains any NULL value

tweet.shape

tweet.columns

tweet.head(10)

tweet["SentimentText"][5]

tweet.info()

import seaborn as sns
sns.countplot(x='Sentiment', data=tweet) # A little bit more of inspection here.
# We can see there are more positive sentiment texts than negative ones

"""According to the data inspection, this tweets_train_validation.csv file has around 100,000 tweets in total. Each tweet has two columns: sentiment text and the corresponding sentiment label (0: negative, 1: positive).

# 2. Data Preprocessing
"""

cleaned_sen = [] # Created a cleaned sentences to store the sentiment texts after the clean process
sentences = tweet.SentimentText.values
preprocess = re.compile("(https*\S+)|(@\S+)|(#\S+)|(\'\w+)|([^\w\s])|(\w*\d+\w*)|(\s{2,})") # Process for removing tags and symbols

def preprocess_text(sen):
  sentences = remove_tags(sen) # Remove html tags, in our case there seems no such tags
  sentences = re.sub('[^a-zA-Z]', ' ', sentences) # Remove punctuations and numbers
  sentences = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentences) # Remove single character
  sentences = re.sub(r'\s+', ' ', sentences) # Remove multiple spaces
  return sentences
import re
TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return TAG_RE.sub('', text)

for text in sentences:
  cleaned_line = preprocess.sub(' ', text) # Recall the preprocess line above
  cleaned_line = cleaned_line.strip().lower().encode('ascii', 'ignore').decode() # Converted to lower cases, remove extra spaces, etc
  cleaned_sen.append(cleaned_line)

cleaned_sen[:20] # From the output, we can see that the tags, puncuations and numbers have been mostly removed. We are only left with alphabets

Y = tweet['Sentiment']
# Y = np.array(list(map(lambda x:'postive' if x==1 else 'negative', Y)))
Y[2]

from sklearn.feature_extraction.text import TfidfVectorizer
# The script below uses tfidf and then divides our data into 80% for the training set and 20% for the testing set
vectorizer = TfidfVectorizer(stop_words='english', max_features=8000, ngram_range=(1,1))
X_train = vectorizer.fit_transform(cleaned_sen)[:80000].A 
Y_train = tweet.Sentiment[:80000] 
X_test = vectorizer.fit_transform(cleaned_sen)[80000:].A
Y_test = tweet.Sentiment[80000:]
# The train set will be used to train our learning model while the testing set will be used to evalute hoe well our model performs

"""#3. Define and Compile Model

Models in Keras are defined as a sequence of layers. In this lab, we will use a fully-connected network structure.
"""

model = Sequential()
# model.add(Dense(12,input_dim=8, activation='relu'))
model.add(Dense(8,activation='relu'))
model.add(Dense(1,activation='sigmoid')) # Could test different embedding layer by editing this part

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

"""# 4. Fit Model"""

model.fit(X_train,Y_train, epochs=15, batch_size=2000) 
# We use fit method to train our fully connected neural network. At the end of training, we can see that the training accuracy is around 78%

"""#5. Evaluate Model"""

# To evaluate the performance of the model, we can simply pass the test to the evaluate method of our model.
# To check the test accuracy and loss, execute the script and we can see that we get a test accuracy of 74%
# Our training accuaracy was 78%. This means that our model is slightly overfitting on the training set.
scores = model.evaluate(X_test, Y_test)
# print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))